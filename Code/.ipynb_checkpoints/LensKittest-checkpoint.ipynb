{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lenskit.util import init_rng\n",
    "from lenskit.batch import predict, recommend, MultiEval\n",
    "from lenskit.crossfold import partition_users,partition_rows,sample_rows, SampleN, SampleFrac\n",
    "from lenskit.algorithms import basic,als,svd,funksvd, user_knn, item_knn\n",
    "from lenskit.datasets import MovieLens, ML100K\n",
    "from lenskit import topn, util, Recommender\n",
    "from lenskit.metrics.predict import rmse, mae\n",
    "from myItemBasedImplementation import MyItemBasedImplementation\n",
    "#from bbcf import BiclusterBasedCF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelgarcao/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   INFO] lenskit.util.log notebook logging configured\n"
     ]
    }
   ],
   "source": [
    "util.log_to_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data - ML-20M, ML-Latest, ML-Latest-Small, ML100K, ML1M, ML10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlsmall = MovieLens('../Datasets/ml-latest-small')\n",
    "mlsmall.ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Personalized Mean Rating Prediction\n",
    "algo_bias = basic.Bias()\n",
    "algo_random = basic.Random(rng_spec=99)\n",
    "algo_pop = basic.Popular()\n",
    "algo_knnuu = user_knn.UserUser(nnbrs=20)\n",
    "algo_knnii = item_knn.ItemItem(nnbrs=20)\n",
    "##pure svd\n",
    "algo_svd = svd.BiasedSVD(20)\n",
    "## iterative SVD\n",
    "algo_biasedmf = als.BiasedMF(20)\n",
    "algo_implicitmf = als.ImplicitMF(20)\n",
    "##funk svd\n",
    "algo_funksvd =funksvd.FunkSVD(20,random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-based splitting - making sure each user is tested with the same number of ratings.\n",
    "100836/5 = 20167 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = MultiEval('../Results/ml-latest-small', recommend=20)\n",
    "#crossfold rows - \n",
    "crossfold_rows = list(partition_rows(mlsmall.ratings, 10,rng_spec=99))\n",
    "eval.add_datasets(crossfold_rows, name=\"ML-Small-crossvalrows-10folds\")\n",
    "#splitted data\n",
    "crossfold_users = list(partition_users(mlsmall.ratings, 10, SampleN(10),rng_spec=99))\n",
    "eval.add_datasets(crossfold_users, name='ML-Small-crossvalusers-10folds-10users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta a dar erro random\n",
    "#eval.add_algorithms(Recommender.adapt(algo_random), name=\"Random\")\n",
    "eval.add_algorithms(algo_bias, name=\"Bias\")\n",
    "eval.add_algorithms(algo_pop, name='Pop')\n",
    "eval.add_algorithms(algo_biasedmf, name='BiasedMF')\n",
    "eval.add_algorithms(algo_implicitmf, name='ImplicitMF')\n",
    "eval.add_algorithms(algo_funksvd,name=\"Funksvd\")\n",
    "#esta a dar erro svd\n",
    "#eval.add_algorithms(algo_svd,name=\"Svd\")\n",
    "eval.add_algorithms(Recommender.adapt(algo_knnuu),name=\"Knn-User\")\n",
    "eval.add_algorithms(Recommender.adapt(algo_knnii),name=\"Knn-Item\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval.run(progress=tqdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runs = pd.read_csv('../Results/ml-latest-small/runs.csv')\n",
    "runs.set_index('RunId', inplace=True)\n",
    "runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.read_parquet('../Results/ml-latest-small/recommendations.parquet')\n",
    "recs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_rows = pd.concat((p.test for p in crossfold_rows), ignore_index=True)\n",
    "ground_truth_users = pd.concat((p.test for p in crossfold_users), ignore_index=True)\n",
    "\n",
    "ground_truth_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rla = topn.RecListAnalysis()\n",
    "rla.add_metric(topn.recall)\n",
    "rla.add_metric(topn.precision)\n",
    "rla.add_metric(topn.ndcg)\n",
    "raw_results_rows = rla.compute(recs, ground_truth_rows)\n",
    "raw_results_users = rla.compute(recs, ground_truth_users)\n",
    "\n",
    "raw_results_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rows = raw_results_rows.join(runs[['name']], on='RunId')\n",
    "results_users = raw_results_users.join(runs[['name']], on='RunId')\n",
    "results_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the overall average performance for each algorithm configuration - fillna makes the group-by happy with Popular's lack of a feature count:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rows.fillna(0).groupby(['name'])[['precision','recall','ndcg']].mean()\n",
    "results_users.fillna(0).groupby(['name'])[['precision','recall','ndcg']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rows.fillna(0).groupby(['name'])[['precision','recall','ndcg']].mean().plot.bar()\n",
    "results_users.fillna(0).groupby(['name'])[['precision','recall','ndcg']].mean().plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste do MyAlgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   INFO] lenskit.util.random initialized LensKit RNG with seed SeedSequence(\n",
      "    entropy=99,\n",
      ")\n",
      "[   INFO] lenskit.util.random initializing numpy.random and random with seed 3328269970\n"
     ]
    }
   ],
   "source": [
    "init_rng(99)\n",
    "mlsmall = ML100K('../Datasets/ml-100k')\n",
    "ratings = mlsmall.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_dict = dict()\n",
    "algo_useruser = user_knn.UserUser(nnbrs=30,min_nbrs=1,min_sim=0.0000001,center=False)\n",
    "algo_dict[\"UserUser-cosine\"] = algo_useruser\n",
    "algo_useruser_center = user_knn.UserUser(nnbrs=30,min_nbrs=1,min_sim=0.0000001)\n",
    "algo_dict[\"UserUser-meancentered\"] = algo_useruser_center\n",
    "\n",
    "algo_itemitem = item_knn.ItemItem( nnbrs=10,min_nbrs=1,min_sim=0.0000001,center=False)\n",
    "algo_dict[\"ItemItem-cosine\"] = algo_itemitem\n",
    "algo_itemitem_center = item_knn.ItemItem( nnbrs=10,min_nbrs=1,min_sim=0.0000001)\n",
    "algo_dict[\"ItemItem-meancentered\"] = algo_itemitem_center\n",
    "algo_ibknn_adjustedcosine = MyItemBasedImplementation(nnbrs=10,min_nbrs=1,min_sim=0.0000001, sim_metric= \"adjusted_cosine\")\n",
    "algo_dict[\"MIBKNN-adjustedcosine\"] = algo_ibknn_adjustedcosine\n",
    "algo_ibknn_cosine_corates_sarwar = MyItemBasedImplementation(nnbrs=10,min_nbrs=1,min_sim=0.0000001,sarwar=True, sim_metric= \"cosine_corates\")\n",
    "algo_dict[\"MIBKNN-cosine-corates-sarwar\"] = algo_ibknn_cosine_corates_sarwar\n",
    "algo_ibknn_adjustedcosine_sarwar = MyItemBasedImplementation(nnbrs=10,min_nbrs=1,min_sim=0.0000001,sarwar=True, sim_metric= \"adjusted_cosine\")\n",
    "algo_dict[\"MIBKNN-adjustedcosine-sarwar\"] = algo_ibknn_adjustedcosine_sarwar   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prediction(aname, algo, train, test):\n",
    "\n",
    "    model = algo.fit(train)\n",
    "    pred = predict(model, test)\n",
    "    # add the algorithm\n",
    "    pred['Algorithm'] = aname\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-based 5-fold cross-validation with 5 test rows per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = dict()\n",
    "test_data = []\n",
    "for train, test in partition_users(ratings, 1, SampleN(5), rng_spec = 99):\n",
    "    test_data.append(test)\n",
    "    for name, algorithm in algo_dict.items():\n",
    "        print(name)\n",
    "        all_preds.setdefault(name, []).append(eval_prediction(name, algorithm, train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy_results(results):\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    for partition in results:\n",
    "        rmse_scores.append(partition.groupby('user').apply(lambda df: rmse(df.prediction, df.rating)).mean())\n",
    "        mae_scores.append(partition.groupby('user').apply(lambda df: mae(df.prediction, df.rating)).mean())\n",
    "    return np.mean(rmse_scores), np.mean(mae_scores), rmse_scores, mae_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for algo_name in all_preds.keys():\n",
    "    print(algo_name + \":\")\n",
    "    print(\"rmse - \"  + str(round(eval_accuracy_results(all_preds[algo_name])[0],3)) +\n",
    "         \", mae - \" + str(round(eval_accuracy_results(all_preds[algo_name])[1],3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_coverage(results_algo):\n",
    "    num_nans = list()\n",
    "    total_nans = 0\n",
    "    total_rows = 0\n",
    "    for partition in results_algo:\n",
    "        nans = partition.prediction.isna().sum()\n",
    "        rows = len(partition)\n",
    "        num_nans.append((nans,rows))\n",
    "        total_nans += nans\n",
    "        total_rows += rows\n",
    "    return (total_rows-total_nans)/total_rows, num_nans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COVERAGE EVAL\")\n",
    "for algo_name in all_preds.keys():\n",
    "    print(algo_name + \":\")\n",
    "    print(\"coverage - \" + str(round(eval_coverage(all_preds[algo_name])[0],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo_name in all_preds.keys():\n",
    "    print(algo_name + \":\")\n",
    "    print(\"coverage - \" + str(eval_coverage(all_preds[algo_name])[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analise BBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
